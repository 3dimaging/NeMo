# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2020, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.10.0b10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-02 10:41-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../nlp/neural_machine_translation.rst:2
msgid "Tutorial"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:4
msgid ""
"In this tutorial we are going to implement Neural Machine Translation "
"(NMT) system based on `Transformer encoder-decoder architecture "
"<https://arxiv.org/abs/1706.03762>`_ :cite:`nlp-nmt-"
"vaswani2017attention`. All code used in this tutorial is based on "
"``examples/nlp/neural_machine_translation/machine_translation_tutorial.py``."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:9
msgid "Preliminaries"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:11
msgid ""
"**Dataset.** We use WMT16 English-German dataset which consists of "
"approximately 4.5 million sentence pairs before preprocessing. To clean "
"the dataset we remove all sentence pairs such that:"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:14
msgid ""
"The length of either source or target is greater than 128 or smaller than"
" 3 tokens."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:15
msgid "Absolute difference between source and target is greater than 25 tokens."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:16
msgid "One sentence is more than 2.5 times longer than the other."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:17
msgid ""
"Target sentence is the exact copy of the source sentence :cite:`nlp-nmt-"
"ott2018analyzing`."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:19
msgid ""
"We use newstest2013 for development and newstest2014 for testing. All "
"datasets, as well as the tokenizer model can be downloaded from `here "
"<https://drive.google.com/open?id=1AErD1hEg16Yt28a-IGflZnwGTg9O27DT>`__. "
"In the following steps, we assume that all data is located at "
"**<path_to_data>**."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:22
msgid ""
"**Resources.** Training script "
"``examples/nlp/neural_machine_translation/machine_translation_tutorial.py``"
" used in this tutorial allows to train Transformer-big architecture to "
"**29.2** BLEU / **28.5** SacreBLEU on newstest2014 in approximately 15 "
"hours on NVIDIA's DGX-1 with 16GB Volta GPUs. This setup can also be "
"replicated with fewer resources by using more steps of gradient "
"accumulation :cite:`nlp-nmt-ott2018scaling`."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:27
msgid ""
"Launching training script without any arguments will run training on much"
" smaller dataset (newstest2013) of 3000 sentence pairs and validate on "
"the subset of this dataset consisting of 100 sentence pairs. This is "
"useful for debugging purposes: if everything is set up correctly, "
"validation BLEU will reach >99 and training / validation losses will go "
"to <1.5 pretty fast."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:32
msgid "Code overview"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:34
msgid ""
"First of all, we instantiate Neural Module Factory which defines 1) "
"backend, 2) mixed precision optimization level, and 3) local rank of the "
"GPU."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:45
msgid ""
"We define tokenizer which allows to transform input text into tokens. In "
"this tutorial, we use joint `Byte Pair Encodings (BPE) "
"<https://arxiv.org/abs/1508.07909>`_ :cite:`nlp-nmt-sennrich2015neural` "
"trained on WMT16 En-De corpus with `YouTokenToMe library "
"<https://github.com/VKCOM/YouTokenToMe>`_. In contrast to the models "
"presented in the literature (which usually have vocabularies of size "
"30000+), we work with 4x smaller vocabulary of 8192 BPEs. It achieves the"
" same level of performance but allows to increase the batch size by 20% "
"which in turn leads to faster convergence."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:59
msgid ""
"To leverage the best GPU utilization and mixed precision speedup, make "
"sure that the vocabulary size (as well as all sizes in the model) is "
"divisible by 8."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:61
msgid ""
"If the source language differs from the target language a lot, then we "
"should use different tokenizers for them. For example, if the source "
"language is English and the target language is Chinese, we can use "
"YouTokenToMeTokenizer for source and CharTokenizer for target. This means"
" the input of the model are English BPEs and the output of the model are "
"Chinese characters."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:72
msgid ""
"You should pass the path of the vocabulary file to the CharTokenizer. The"
" vocabulary file should contain the characters of the corresponding "
"language."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:74
msgid "Next, we define all Neural Modules necessary for our model:"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:76
msgid "Transformer Encoder and Decoder."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:77
msgid ""
"`TokenClassifier` for mapping output of the decoder into probability "
"distribution over vocabulary."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:78
msgid "Beam Search module for generating translations."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:79
msgid "Loss function (cross entropy with label smoothing regularization)."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:89
msgid ""
"Following `Press and Wolf, 2016 <https://arxiv.org/abs/1608.05859>`_ "
":cite:`nlp-nmt-press2016using`, we also tie the parameters of embedding "
"and softmax layers:"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:112
msgid ""
"You should not tie the parameters if you use different tokenizers for "
"source and target."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:114
msgid ""
"Then, we create the pipeline gtom input to output that can be used for "
"both training and evaluation. An important element of this pipeline is "
"the datalayer that packs input sentences into batches of similar length "
"to minimize the use of padding symbol. Note, that the maximum allowed "
"number of tokens in a batch is given in **source and target** tokens."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:150
msgid "Next, we define necessary callbacks:"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:152
msgid "`SimpleLossLoggerCallback`: tracking loss during training"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:153
msgid ""
"`EvaluatorCallback`: tracking BLEU score on evaluation dataset at set "
"intervals"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:154
msgid "`CheckpointCallback`: saving model checkpoints"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:166
msgid ""
"The BLEU score is calculated between detokenized translation (generated "
"with beam search) and genuine evaluation dataset. For the sake of "
"completeness, we report both  `SacreBLEU "
"<https://github.com/mjpost/sacreBLEU>`_ :cite:`nlp-nmt-post2018call` and "
"`tokenized BLEU score <https://github.com/moses-"
"smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl>`_ commonly "
"used in the literature."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:170
msgid "Finally, we define the optimization parameters and run the whole pipeline."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:190
msgid "Model training"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:192
msgid ""
"To train the Transformer-big model, run "
"``machine_translation_tutorial.py`` located at "
"``examples/nlp/neural_machine_translation``:"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:206
msgid ""
"This command runs training on 8 GPUs with at least 16 GB of memory. If "
"your GPUs have less memory, decrease the **batch_size** parameter. To "
"train with bigger batches which do not fit into the memory, increase the "
"**iter_per_step** parameter."
msgstr ""

#: ../../nlp/neural_machine_translation.rst:210
msgid "Translation with pretrained model"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:212
msgid ""
"1. Put your saved checkpoint (or download good checkpoint which obtains "
"28.5 SacreBLEU on newstest2014 from `here "
"<https://ngc.nvidia.com/catalog/models/nvidia:transformer_big_en_de_8k>`__)"
" into **<path_to_ckpt>**. 2. Run ``machine_translation_tutorial.py`` in "
"an interactive mode::"
msgstr ""

#: ../../nlp/neural_machine_translation.rst:226
#: ../../nlp/neural_machine_translation.rst:229
msgid "References"
msgstr ""

