# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2020, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.10.0b10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-02 10:41-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../nlp/punctuation.rst:2
msgid "Tutorial"
msgstr ""

#: ../../nlp/punctuation.rst:5
msgid ""
"An ASR system typically generates text with no punctuation and "
"capitalization of the words. This tutorial explains how to implement a "
"model in NeMo that will predict punctuation and capitalization for each "
"word in a sentence to make ASR output more readable and to boost "
"performance of the downstream tasks such as name entity recognition or "
"machine translation. We'll show how to train network for this task using "
"a pre-trained BERT model."
msgstr ""

#: ../../nlp/punctuation.rst:9
msgid ""
"We recommend you to try this example in Jupyter notebook "
"examples/nlp/token_classification/PunctuationWithBERT.ipynb."
msgstr ""

#: ../../nlp/punctuation.rst:11
msgid ""
"All code used in this tutorial is based on :ref:`punct_scripts`. For "
"pretraining BERT in NeMo and pretrained model checkpoints go to `BERT "
"pretraining <https://nvidia.github.io/NeMo/nlp/bert_pretraining.html>`__."
msgstr ""

#: ../../nlp/punctuation.rst:16
msgid "Task Description"
msgstr ""

#: ../../nlp/punctuation.rst:18
msgid "For every word in our training dataset we're going to predict:"
msgstr ""

#: ../../nlp/punctuation.rst:20
msgid "punctuation mark that should follow the word and"
msgstr ""

#: ../../nlp/punctuation.rst:21
msgid "whether the word should be capitalized"
msgstr ""

#: ../../nlp/punctuation.rst:23
msgid ""
"In this model, we're jointly training 2 token-level classifiers on top of"
" the pretrained BERT model: one classifier to predict punctuation and the"
" other one - capitalization."
msgstr ""

#: ../../nlp/punctuation.rst:26
msgid "Dataset"
msgstr ""

#: ../../nlp/punctuation.rst:28
msgid ""
"This model can work with any dataset as long as it follows the format "
"specified below. For this tutorial, we're going to use the `Tatoeba "
"collection of sentences`_. `This`_ script downloads and preprocesses the "
"dataset."
msgstr ""

#: ../../nlp/punctuation.rst:34
msgid ""
"The training and evaluation data is divided into 2 files: text.txt and "
"labels.txt. Each line of the text.txt file contains text sequences, where"
" words are separated with spaces: [WORD] [SPACE] [WORD] [SPACE] [WORD], "
"for example:"
msgstr ""

#: ../../nlp/punctuation.rst:43
msgid ""
"The labels.txt file contains corresponding labels for each word in "
"text.txt, the labels are separated with spaces. Each label in labels.txt "
"file consists of 2 symbols:"
msgstr ""

#: ../../nlp/punctuation.rst:46
msgid ""
"the first symbol of the label indicates what punctuation mark should "
"follow the word (where ``O`` means no punctuation needed);"
msgstr ""

#: ../../nlp/punctuation.rst:47
msgid ""
"the second symbol determines if the word needs to be capitalized or not "
"(where ``U`` indicates that the associated with this label word should be"
" upper cased, and ``O`` - no capitalization needed.)"
msgstr ""

#: ../../nlp/punctuation.rst:49
msgid ""
"We're considering only commas, periods, and question marks for this task;"
" the rest punctuation marks were removed. Each line of the labels.txt "
"should follow the format: [LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for "
"labels.txt). For example, labels for the above text.txt file should be:"
msgstr ""

#: ../../nlp/punctuation.rst:59
msgid ""
"The complete list of all possible labels for this task is: ``OO``, "
"``,O``, ``.O``, ``?O``, ``OU``, ``,U``, ``.U``, ``?U``."
msgstr ""

#: ../../nlp/punctuation.rst:62
msgid "Code overview"
msgstr ""

#: ../../nlp/punctuation.rst:64
msgid ""
"First, let's set some parameters that we're going to need through out "
"this tutorial:"
msgstr ""

#: ../../nlp/punctuation.rst:85
msgid ""
"To download and preprocess a subset of the Tatoeba collection of "
"sentences, run:"
msgstr ""

#: ../../nlp/punctuation.rst:91
msgid ""
"Then, we need to create our neural factory with the supported backend. "
"This tutorial assumes that you're training on a single GPU, with mixed "
"precision (``optimization_level=\"O1\"``). If you don't want to use mixed"
" precision, set ``optimization_level`` to ``O0``."
msgstr ""

#: ../../nlp/punctuation.rst:101
msgid ""
"Next, we'll need to define our tokenizer and our BERT model. Currently, "
"there are 3 pretrained back-bone models supported: BERT, ALBERT and "
"RoBERTa. These are pretrained model checkpoints from `transformers "
"<https://huggingface.co/transformers>`__ . Apart from these, the user can"
" also do fine-tuning on a custom BERT checkpoint, specified by the "
"`--bert_checkpoint` argument in the training script. The pretrained back-"
"bone models can be specified `--pretrained_model_name`. See the list of "
"available pre-trained models by calling "
"`nemo.collections.nlp.nm.trainables.get_bert_models_list()`. \\"
msgstr ""

#: ../../nlp/punctuation.rst:113
msgid "Now, create the train and evaluation data layers:"
msgstr ""

#: ../../nlp/punctuation.rst:143
msgid ""
"Now, create punctuation and capitalization classifiers to sit on top of "
"the pretrained BERT model and define the task loss function:"
msgstr ""

#: ../../nlp/punctuation.rst:170
msgid ""
"Below, we're passing the output of the datalayers through the pretrained "
"BERT model and to the classifiers:"
msgstr ""

#: ../../nlp/punctuation.rst:204
msgid "Now, we will set up our callbacks. We will use 3 callbacks:"
msgstr ""

#: ../../nlp/punctuation.rst:206
msgid "`SimpleLossLoggerCallback` prints loss values during training;"
msgstr ""

#: ../../nlp/punctuation.rst:207
msgid ""
"`EvaluatorCallback` calculates the performance metrics for the dev "
"dataset;"
msgstr ""

#: ../../nlp/punctuation.rst:208
msgid "`CheckpointCallback` is used to save and restore checkpoints."
msgstr ""

#: ../../nlp/punctuation.rst:240
msgid ""
"Finally, we'll define our learning rate policy and our optimizer, and "
"start training:"
msgstr ""

#: ../../nlp/punctuation.rst:256
msgid "Inference"
msgstr ""

#: ../../nlp/punctuation.rst:258
msgid ""
"To see how the model performs, let's run inference on a few samples. We "
"need to define a data layer for inference the same way we created data "
"layers for training and evaluation."
msgstr ""

#: ../../nlp/punctuation.rst:274
msgid ""
"Run inference, append punctuation and capitalize words based on the "
"generated predictions:"
msgstr ""

#: ../../nlp/punctuation.rst:324
msgid "Inference results:"
msgstr ""

#: ../../nlp/punctuation.rst:346
msgid "Training and inference scripts"
msgstr ""

#: ../../nlp/punctuation.rst:348
msgid "To run the provided training script:"
msgstr ""

#: ../../nlp/punctuation.rst:354
msgid "To run inference:"
msgstr ""

#: ../../nlp/punctuation.rst:360
msgid ""
"Note, punct_label_ids.csv and capit_label_ids.csv files will be generated"
" during training and stored in the data_dir folder."
msgstr ""

#: ../../nlp/punctuation.rst:363
msgid "Multi GPU Training"
msgstr ""

#: ../../nlp/punctuation.rst:365
msgid "To run training on multiple GPUs, run"
msgstr ""

