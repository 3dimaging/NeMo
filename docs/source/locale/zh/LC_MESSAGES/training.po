# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2020, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.10.0b10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-02 10:41-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../training.rst:2
msgid "Fast Training"
msgstr ""

#: ../../training.rst:4
msgid ""
"Training a large model (especially from scratch) requires significant "
"compute. NeMo provides support for mixed precision and distributed "
"training to speed-up training. NeMo uses `NVIDIA's APEX library "
"<https://github.com/NVIDIA/apex>`_ to get maximum performance out of "
"NVIDIA's GPUs. Furthermore, multi-GPU systems (such as DGX Station, DGX-1"
" and DGX-2) have *NVLINK* to speed-up multi-GPU communication."
msgstr ""

#: ../../training.rst:8
msgid "Mixed Precision"
msgstr ""

#: ../../training.rst:9
msgid ""
"NVIDIA Volta and Turing GPUs have *Tensor Cores* which can do fast matrix"
" multiplications with values in float16 format. To enable mixed-precision"
" in NeMo all you need to do is to set `optimization_level` parameter of "
"`nemo.core.NeuralModuleFactory` to `nemo.core.Optimization.mxprO1`. For "
"example:"
msgstr ""

#: ../../training.rst:18
msgid ""
"Mixed precision requires Tensor Cores, so it works only on NVIDIA Volta "
"and Turing GPUs."
msgstr ""

#: ../../training.rst:21
msgid "Multi-GPU Training"
msgstr ""

#: ../../training.rst:23
msgid "For multi-GPU training:"
msgstr ""

#: ../../training.rst:25
msgid ""
"Add 'local_rank' argument to your script and do not set it yourself: "
"`parser.add_argument(\"--local_rank\", default=os.getenv('LOCAL_RANK', "
"None), type=int)`"
msgstr ""

#: ../../training.rst:33
msgid ""
"Use `torch.distributed.launch` package to run your script like this "
"(assuming 8 GPUs):"
msgstr ""

#: ../../training.rst:40
msgid "Example"
msgstr ""

#: ../../training.rst:42
msgid ""
"Please refer to the `<nemo_git_repo_root>/examples/asr/jasper.py` for a "
"comprehensive example. It builds one train DAG and up to three validation"
" DAGs to evaluate on different datasets."
msgstr ""

#: ../../training.rst:45
msgid "If you are working with a Volta-based DGX, you can run training like this:"
msgstr ""

#: ../../training.rst:51
msgid ""
"The command above should trigger 8-GPU training with mixed precision. In "
"the command above various manifests (.json) files are various datasets. "
"Substitute them with the ones containing your data."
msgstr ""

#: ../../training.rst:54
msgid ""
"You can pass several manifests (comma-separated) to train on a combined "
"dataset like this: `--train_manifest=/manifests/librivox-train-"
"all.json,/manifests/librivox-train-all-"
"sp10pcnt.json,/manifests/cv/validated.json`."
msgstr ""

#: ../../training.rst:56
msgid ""
"This example would train on 3 data sets: LibriSpeech, Mozilla Common "
"Voice and LibriSpeech speed perturbed."
msgstr ""

#: ../../training.rst:59
msgid "Multi-Node Training"
msgstr ""

#: ../../training.rst:60
msgid ""
"We highly recommend reading pytorch's distributed documentation prior to "
"trying multi-node, but here is a quick start guide on how to setup multi-"
"node training using TCP initialization. Assume that we have 2 machines "
"each with 4 gpus each. Let's call machine 1 the master node. We need the "
"IP address of the master node and a free port on the master node. On "
"machine 1, we run"
msgstr ""

#: ../../training.rst:69
msgid "On machine 2, we run"
msgstr ""

#: ../../training.rst:76
msgid ""
"Setting the environment variable NCCL_DEBUG to INFO can help identify "
"setup issues"
msgstr ""

#: ../../training.rst:79
msgid ""
"We recommend reading the following pytorch documentation "
"https://pytorch.org/docs/stable/distributed.html#launch-utility "
"https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py"
msgstr ""

#: ../../training.rst:84
msgid ""
"To help with multi-processing, neural_factory contains two attributes: "
"``local_rank`` and ``global_rank``. ``local_rank`` refers to the rank on "
"the current machine whereas ``global_rank`` refers to the rank across all"
" machines. For example, assume you have 2 machines each with 4 gpus. "
"global_rank 0 will have local_rank 0 and have the 1st gpu on machine 1, "
"whereas global_rank 5 COULD have local_rank 0 and have the 1st gpu on "
"machine 2. In other words local_rank == 0 and global_rank == 0 ensures "
"that it has the 1st GPU on the master node, and local_rank == 0 and "
"global_rank != 0 ensures that it has the 1st GPU on slave nodes."
msgstr ""

