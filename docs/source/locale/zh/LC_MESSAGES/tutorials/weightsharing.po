# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2020, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.10.0b10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-02 10:41-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../tutorials/weightsharing.rst:2
msgid "Weight Sharing between Modules"
msgstr ""

#: ../../tutorials/weightsharing.rst:4
msgid "There are several ways to share or tie weights between neural modules."
msgstr ""

#: ../../tutorials/weightsharing.rst:7
msgid "Neural Module Reuse"
msgstr ""

#: ../../tutorials/weightsharing.rst:9
msgid ""
"The idea is to re-use neural modules between training, evaluation and "
"inference graphs. For example:"
msgstr ""

#: ../../tutorials/weightsharing.rst:40
msgid "Weight Copying Between Modules"
msgstr ""

#: ../../tutorials/weightsharing.rst:41
msgid ""
":class:`NeuralModule<nemo.core.neural_modules.NeuralModule>` class "
"provides 2 methods "
":meth:`get_weights<nemo.core.neural_modules.NeuralModule.get_weights>` "
"and "
":meth:`set_weights<nemo.core.neural_modules.NeuralModule.set_weights>` "
"for copying weights."
msgstr ""

#: ../../tutorials/weightsharing.rst:47
msgid ""
":meth:`set_weights<nemo.core.neural_modules.NeuralModule.set_weights>` "
"method can set only part of module's weights."
msgstr ""

#: ../../tutorials/weightsharing.rst:50
msgid ""
"This approach is used only to copy weights. Subsequent update of weights "
"in one module will not affect weights in the other module. This means "
"that the weights will get DIFFERENT gradients on the update step."
msgstr ""

#: ../../tutorials/weightsharing.rst:53
msgid "Consider an example:"
msgstr ""

#: ../../tutorials/weightsharing.rst:77
msgid "Weight Tying Between Modules"
msgstr ""

#: ../../tutorials/weightsharing.rst:78
msgid ""
":class:`NeuralModule<nemo.core.neural_modules.NeuralModule>` class "
"provides "
":meth:`tie_weights_with<nemo.core.neural_modules.NeuralModule.tie_weights_with>`"
" method to tie weights between two or more modules."
msgstr ""

#: ../../tutorials/weightsharing.rst:81
msgid ""
"Tied weights are identical across all modules. Gradients to the weights "
"will be the SAME."
msgstr ""

#: ../../tutorials/weightsharing.rst:84
msgid ""
"However manually updating the weight on one module via tensor.data will "
"NOT update the weight on the other module"
msgstr ""

#: ../../tutorials/weightsharing.rst:86
msgid ""
"In the example below we first create a simple embedding encoder which "
"takes [batch, time] sequences of word ids from vocabulary ``V``  and "
"embeds them into some ``D``-dimensional space. Effectively, this is a "
"lookup-based projection from ``V``-dimensional space to ``D``-dimensional"
" space. We then create a decoder which projects from ``D``-dimensional "
"space back to the ``V``-dimensional space. We want to transpose the "
"encoder projection matrix and reuse it for decoder. The code below "
"demonstrates how this can be achieved."
msgstr ""

#: ../../tutorials/weightsharing.rst:90
msgid ""
"The weights have different names (``embedding.weight`` and "
"``projection.weight``) but their values and gradient updates will be the "
"same."
msgstr ""

#: ../../tutorials/weightsharing.rst:109
msgid ""
"Manually setting the weight tensors to be equal to the other will likely "
"break multi-GPU and multi-node runs. Eg, ``embd.embedding.weight = "
"proj.projection.weights`` is not recommended. Use the "
"``tie_weights_with()`` function instead"
msgstr ""

