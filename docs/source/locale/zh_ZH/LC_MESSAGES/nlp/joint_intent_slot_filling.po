# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2020, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.10.0b10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-02 10:41-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../nlp/joint_intent_slot_filling.rst:2
msgid "Tutorial"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:4
msgid ""
"In this tutorial, we are going to show the structure of our example on "
"training and evaluating an intent detection and slot filling model with "
"pretrained BERT model. \\ This model is based on a model proposed in "
"`BERT for Joint Intent Classification and Slot Filling "
"<https://arxiv.org/abs/1902.10909>`_ :cite:`nlp-slot-chen2019bert`. All "
"the code introduced in this tutorial is based on "
"``examples/nlp/intent_detection_slot_tagging/joint_intent_slot_with_bert.py``."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:8
msgid ""
"There are a variety pre-trained BERT models that we can select as the "
"base encoder for our model. We're currently using the script for loading "
"pre-trained models from `transformers`. \\ See the list of available pre-"
"trained models by calling "
"`nemo.collections.nlp.nm.trainables.get_bert_models_list()`. \\ The type "
"of the encoder can get defined by the argument `--pretrained_model_name`."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:15
msgid ""
"For pretraining BERT model in NeMo and also downloading pretrained model "
"checkpoints go to `BERT pretraining "
"<https://nvidia.github.io/NeMo/nlp/bert_pretraining.html>`__."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:19
msgid "Preliminaries"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:21
msgid ""
"**Model details** This model jointly train the sentence-level classifier "
"for intents and token-level classifier for slots by minimizing the "
"combined loss of the two classifiers:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:24
msgid "intent_loss * intent_loss_weight + slot_loss * (1 - intent_loss_weight)"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:26
msgid "When `intent_loss_weight = 0.5`, this loss jointly maximizes:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:28
msgid "p(y | x)P(s1, s2, ..., sn | x)"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:30
msgid ""
"with x being the sequence of n tokens (x1, x2, ..., xn), y being the "
"predicted intent for x, and s1, s2, ..., sn being the predicted slots "
"corresponding to x1, x2, ..., xn."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:32
msgid "**Datasets.**"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:36
msgid "This model can work with any dataset that follows the NeMo's format:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:35
msgid ""
"input file: a `tsv` file with the first line as a header "
"[sentence][tab][label]"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:36
msgid ""
"slot file: slot labels for all tokens in the sentence, separated by "
"space. The length of the slot labels should be the same as the length of "
"all tokens in sentence in input file."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:38
msgid ""
"Datasets which are not in this format should get processed and converted "
"into NeMo's format. \\ Currently, the datasets that we provide pre-"
"processing script for include ATIS which can be downloaded from `Kaggle "
"<https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk>`_ and the "
"SNIPS spoken language understanding research dataset which can be "
"requested from `here <https://github.com/snipsco/spoken-language-"
"understanding-research-datasets>`__. \\"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:43
msgid ""
"You may use "
"``/examples/nlp/intent_detection_slot_tagging/data/import_datasets.py`` "
"script to process these datasets:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:53
msgid ""
"By setting the dataset_name parameter to one of ['atis', 'snips'], you "
"can process and convert these datasets into NeMo's format. you can also "
"write your own preprocessing scripts for any dataset."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:57
msgid "Code Structure"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:59
msgid ""
"First, we instantiate Neural Module Factory which defines 1) backend "
"(PyTorch or TensorFlow), 2) mixed precision optimization level, 3) local "
"rank of the GPU, and 4) an experiment manager that creates a timestamped "
"folder to store checkpoints, relevant outputs, log files, and TensorBoard"
" graphs."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:75
msgid ""
"We define the tokenizer which transforms text into BERT tokens, using a "
"built-in tokenizer by `transformers`. \\ NemoBertTokenizer would select "
"and return the appropriate tokenizer for each model."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:82
msgid ""
"Next, we define all Neural Modules participating in our joint intent slot"
" filling classification pipeline."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:84
msgid ""
"Build data description: the `JointIntentSlotDataDesc` class in "
"`nemo/collections/nlp/data/datasets/joint_intent_slot_dataset/data_descriptor.py`"
" is supposed to do the read the dataset and build its schema."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:94
msgid "Load the pre-trained BERT model to encode the corresponding inputs."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:102
msgid "Create the classifier heads for our task."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:111
msgid ""
"Create loss functions for intent detection and slot filling then and use "
"loss aggregator module to merge them"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:120
msgid ""
"Create the pipelines for the train and evaluation processes. Each "
"pipeline creates its own data layer (BertJointIntentSlotDataLayer)."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:197
msgid ""
"Create relevant callbacks for saving checkpoints, printing training "
"progresses and evaluating results."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:229
msgid "Finally, we define the optimization parameters and run the whole pipeline."
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:247
msgid "Model Training"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:249
msgid ""
"To train an intent detection and slot filling model on a dataset, run "
"``joint_intent_slot_with_bert.py`` located at "
"``examples/nlp/intent_detection_slot_tagging/joint_intent_slot_with_bert.py``:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:258
msgid ""
"By default a folder named \"checkpoints\" would get created under the "
"working folder specified by `--work_dir` and checkpoints are stored under"
" it. To do inference with a checkpoint on test set, you may run:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:269
msgid "To do inference on a single query, run:"
msgstr ""

#: ../../nlp/joint_intent_slot_filling.rst:280
msgid "References"
msgstr ""

