# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018-2020, NVIDIA
# This file is distributed under the same license as the nemo package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nemo 0.10.0b10\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-02 10:41-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../nlp/asr-improvement.rst:2
msgid "Tutorial"
msgstr ""

#: ../../nlp/asr-improvement.rst:4
msgid ""
"In this tutorial we will train an ASR postprocessing model to correct "
"mistakes in output of end-to-end speech recognition model. This model "
"method works similar to translation model in contrast to traditional ASR "
"language model rescoring. The model architecture is attention based "
"encoder-decoder where both encoder and decoder are initialized with "
"pretrained BERT language model. To train this model we collected dataset "
"with typical ASR errors by using pretrained Jasper ASR model :cite:`asr-"
"imps-li2019jasper`."
msgstr ""

#: ../../nlp/asr-improvement.rst:10
msgid "Data"
msgstr ""

#: ../../nlp/asr-improvement.rst:11
msgid ""
"**Data collection.** We collected dataset for this tutorial with Jasper "
"ASR model :cite:`asr-imps-li2019jasper` trained on Librispeech dataset "
":cite:`asr-imps-panayotov2015librispeech`. To download the Librispeech "
"dataset, see :ref:`LibriSpeech_dataset`. To obtain the pretrained Jasper "
"model, see :ref:`Jasper_model`. Librispeech training dataset consists of "
"three parts: train-clean-100, train-clean-360, and train-clean-500 which "
"give 281k training examples in total. To augment this data we used two "
"techniques:"
msgstr ""

#: ../../nlp/asr-improvement.rst:18
msgid ""
"We split all training data into 10 folds and trained 10 Jasper models in "
"cross-validation manner: a model was trained on 9 folds and used to make "
"ASR predictions for the remaining fold."
msgstr ""

#: ../../nlp/asr-improvement.rst:20
msgid ""
"We took pretrained Jasper model and enabled dropout during inference on "
"training data. This procedure was repeated multiple times with different "
"random seeds."
msgstr ""

#: ../../nlp/asr-improvement.rst:22
msgid ""
"**Data postprocessing.** The collected dataset was postprocessed by "
"removing duplicates and examples with word error rate higher than 0.5. "
"The resulting training dataset consists of 1.7M pairs of \"bad\" "
"English-\"good\" English examples."
msgstr ""

#: ../../nlp/asr-improvement.rst:26
msgid ""
"**Dev and test datasets preparation**. Librispeech contains 2 dev "
"datasets (dev-clean and dev-other) and 2 test datasets (test-clean and "
"test-other). For our task we kept the same splits. We fed these datasets "
"to a pretrained Jasper model with the greedy decoding to get the ASR "
"predictions that are used for evaluation in our tutorial."
msgstr ""

#: ../../nlp/asr-improvement.rst:33
msgid "Importing parameters from pretrained BERT"
msgstr ""

#: ../../nlp/asr-improvement.rst:34
msgid ""
"Both encoder and decoder are initialized with pretrained BERT parameters."
" Since BERT language model has the same architecture as transformer "
"encoder, there is no need to do anything additional. To prepare decoder "
"parameters from pretrained BERT we wrote a script "
"``get_decoder_params_from_bert.py`` that downloads BERT parameters from "
"the ``transformers`` repository :cite:`asr-imps-"
"huggingface2019transformers` and maps them into a transformer decoder. "
"Encoder-decoder attention is initialized with self-attention parameters. "
"The script is located under "
"``examples/nlp/asr_postprocessor/get_decoder_params_from_bert.py`` "
"directory and accepts 2 arguments:"
msgstr ""

#: ../../nlp/asr-improvement.rst:41
msgid "``--model_name``: e.g. ``bert-base-cased``, ``bert-base-uncased``, etc."
msgstr ""

#: ../../nlp/asr-improvement.rst:42
msgid "``--save_to``: a directory where the parameters will be saved"
msgstr ""

#: ../../nlp/asr-improvement.rst:50
msgid "Neural modules overview"
msgstr ""

#: ../../nlp/asr-improvement.rst:51
msgid ""
"First, as with all models built in NeMo, we instantiate Neural Module "
"Factory which defines 1) backend (PyTorch), 2) mixed precision "
"optimization level, 3) local rank of the GPU, and 4) an experiment "
"manager that creates a timestamped folder to store checkpoints, relevant "
"outputs, log files, and TensorBoard graphs."
msgstr ""

#: ../../nlp/asr-improvement.rst:65
msgid ""
"Then we define tokenizer to convert tokens into indices. We will use "
"``bert-base-uncased`` vocabulary, since our dataset only contains uncased"
" text:"
msgstr ""

#: ../../nlp/asr-improvement.rst:72
msgid ""
"The encoder block is a neural module corresponding to BERT language model"
" from ``nemo_nlp.nm.trainables.huggingface`` collection:"
msgstr ""

#: ../../nlp/asr-improvement.rst:82
msgid ""
"Making embedding size (as well as all other tensor dimensions) divisible "
"by 8 will help to get the best GPU utilization and speed-up with mixed "
"precision training."
msgstr ""

#: ../../nlp/asr-improvement.rst:97
msgid ""
"Next, we construct transformer decoder neural module. Since we will be "
"initializing decoder with pretrained BERT parameters, we set hidden "
"activation to ``\"hidden_act\": \"gelu\"`` and learn positional encodings"
" ``\"learn_positional_encodings\": True``:"
msgstr ""

#: ../../nlp/asr-improvement.rst:115
msgid ""
"To load the pretrained parameters into decoder, we use ``restore_from`` "
"attribute function of the decoder neural module:"
msgstr ""

#: ../../nlp/asr-improvement.rst:123
msgid "Model training"
msgstr ""

#: ../../nlp/asr-improvement.rst:125
msgid ""
"To train the model run ``asr_postprocessor.py.py`` located in "
"``examples/nlp/asr_postprocessor`` directory. We train with novograd "
"optimizer :cite:`asr-imps-ginsburg2019stochastic`, learning rate "
"``lr=0.001``, polynomial learning rate decay policy, ``1000`` warmup "
"steps, per-gpu batch size of ``4096*8`` tokens, and ``0.25`` dropout "
"probability. We trained on 8 GPUS. To launch the training in multi-gpu "
"mode run the following command:"
msgstr ""

#: ../../nlp/asr-improvement.rst:136
msgid "References"
msgstr ""

